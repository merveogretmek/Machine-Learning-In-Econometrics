{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027c88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this notebook after running Mother's Education and Father's education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e699ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b541153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('meduc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960d704d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           0\n",
      "year         0\n",
      "wage         0\n",
      "hours        0\n",
      "emp          0\n",
      "treat        0\n",
      "female       0\n",
      "IQ           0\n",
      "KWW          0\n",
      "educ         0\n",
      "exper        0\n",
      "tenure       0\n",
      "age          0\n",
      "married      0\n",
      "black        0\n",
      "south        0\n",
      "urban        0\n",
      "sibs         0\n",
      "brthord    188\n",
      "meduc        0\n",
      "feduc        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Number of missing values per column\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb2a0f",
   "metadata": {},
   "source": [
    "# Birth Order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efbd52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.dropna(subset=['brthord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b6085d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id         0\n",
      "year       0\n",
      "wage       0\n",
      "hours      0\n",
      "emp        0\n",
      "treat      0\n",
      "female     0\n",
      "IQ         0\n",
      "KWW        0\n",
      "educ       0\n",
      "exper      0\n",
      "tenure     0\n",
      "age        0\n",
      "married    0\n",
      "black      0\n",
      "south      0\n",
      "urban      0\n",
      "sibs       0\n",
      "brthord    0\n",
      "meduc      0\n",
      "feduc      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(new_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa614e",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e0e26",
   "metadata": {},
   "source": [
    "### Moderate Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4712fe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R2: 0.4047990135880557\n",
      "In-sample MSE: 1.5458971617665145\n",
      "Out-of-sample R2: 0.36172683150172047\n",
      "Out-of-sample MSE: 1.6410282923828774\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model using the training data\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict the target variable for the training and testing data\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "# Calculate in-sample R2 and MSE\n",
    "in_sample_r2 = r2_score(y_train, y_train_pred)\n",
    "in_sample_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate out-of-sample R2 and MSE\n",
    "out_of_sample_r2 = r2_score(y_test, y_test_pred)\n",
    "out_of_sample_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"In-sample R2:\", in_sample_r2)\n",
    "print(\"In-sample MSE:\", in_sample_mse)\n",
    "print(\"Out-of-sample R2:\", out_of_sample_r2)\n",
    "print(\"Out-of-sample MSE:\", out_of_sample_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd22ad",
   "metadata": {},
   "source": [
    "### High Dimension (up to degree 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54dcb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R2 with polynomial features: 0.5392618763936858\n",
      "In-sample MSE with polynomial features: 1.19666091599464\n",
      "Out-of-sample R2 with polynomial features: 0.342603477277278\n",
      "Out-of-sample MSE with polynomial features: 1.6901952742903337\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create polynomial features including all possible interactions up to degree 3\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "x_train_poly = poly_features.fit_transform(x_train)\n",
    "x_test_poly = poly_features.transform(x_test)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model using the training data with polynomial features\n",
    "model.fit(x_train_poly, y_train)\n",
    "\n",
    "# Predict the target variable for the training and testing data with polynomial features\n",
    "y_train_pred = model.predict(x_train_poly)\n",
    "y_test_pred = model.predict(x_test_poly)\n",
    "\n",
    "# Calculate in-sample R2 and MSE with polynomial features\n",
    "in_sample_r2 = r2_score(y_train, y_train_pred)\n",
    "in_sample_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate out-of-sample R2 and MSE with polynomial features\n",
    "out_of_sample_r2 = r2_score(y_test, y_test_pred)\n",
    "out_of_sample_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"In-sample R2 with polynomial features:\", in_sample_r2)\n",
    "print(\"In-sample MSE with polynomial features:\", in_sample_mse)\n",
    "print(\"Out-of-sample R2 with polynomial features:\", out_of_sample_r2)\n",
    "print(\"Out-of-sample MSE with polynomial features:\", out_of_sample_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841e370",
   "metadata": {},
   "source": [
    "## LASSO (alpha = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8b8b9",
   "metadata": {},
   "source": [
    "### Moderate Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecac62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R2: 0.3852634668835623\n",
      "In-sample MSE: 1.5966362346065786\n",
      "Out-of-sample R2: 0.359702742755576\n",
      "Out-of-sample MSE: 1.6462323132671204\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LASSO model\n",
    "model = Lasso(alpha = 0.1)\n",
    "\n",
    "# Fit the model using the training data\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict the target variable for the training and testing data\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "# Calculate in-sample R2 and MSE\n",
    "in_sample_r2 = r2_score(y_train, y_train_pred)\n",
    "in_sample_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate out-of-sample R2 and MSE\n",
    "out_of_sample_r2 = r2_score(y_test, y_test_pred)\n",
    "out_of_sample_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"In-sample R2:\", in_sample_r2)\n",
    "print(\"In-sample MSE:\", in_sample_mse)\n",
    "print(\"Out-of-sample R2:\", out_of_sample_r2)\n",
    "print(\"Out-of-sample MSE:\", out_of_sample_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9470f2c8",
   "metadata": {},
   "source": [
    "### High Dimension (up to degree 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b1bb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R2 with polynomial features: 0.46970634684849555\n",
      "In-sample MSE with polynomial features: 1.3773153472940147\n",
      "Out-of-sample R2 with polynomial features: 0.34469556781484156\n",
      "Out-of-sample MSE with polynomial features: 1.6848164178197635\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create polynomial features including all possible interactions up to degree 3\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "x_train_poly = poly_features.fit_transform(x_train)\n",
    "x_test_poly = poly_features.transform(x_test)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model using the training data with polynomial features\n",
    "model.fit(x_train_poly, y_train)\n",
    "\n",
    "# Predict the target variable for the training and testing data with polynomial features\n",
    "y_train_pred = model.predict(x_train_poly)\n",
    "y_test_pred = model.predict(x_test_poly)\n",
    "\n",
    "# Calculate in-sample R2 and MSE with polynomial features\n",
    "in_sample_r2 = r2_score(y_train, y_train_pred)\n",
    "in_sample_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate out-of-sample R2 and MSE with polynomial features\n",
    "out_of_sample_r2 = r2_score(y_test, y_test_pred)\n",
    "out_of_sample_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"In-sample R2 with polynomial features:\", in_sample_r2)\n",
    "print(\"In-sample MSE with polynomial features:\", in_sample_mse)\n",
    "print(\"Out-of-sample R2 with polynomial features:\", out_of_sample_r2)\n",
    "print(\"Out-of-sample MSE with polynomial features:\", out_of_sample_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1fb08b",
   "metadata": {},
   "source": [
    "## LASSO (Scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109efa0",
   "metadata": {},
   "source": [
    "### Moderate Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c667244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R2 (LASSO with scaled features): 0.3820456022372356\n",
      "In-sample MSE (LASSO with scaled features): 1.6049938951905989\n",
      "Out-of-sample R2 (LASSO with scaled features): 0.3557218576791876\n",
      "Out-of-sample MSE (LASSO with scaled features): 1.6564673432848291\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a StandardScaler object and fit it to the training data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Create a LASSO Regression model\n",
    "model = Lasso(alpha=0.1)  # Adjust the alpha value for regularization strength\n",
    "\n",
    "# Fit the model using the scaled training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable for the training and testing data\n",
    "y_train_pred = model.predict(x_train_scaled)\n",
    "y_test_pred = model.predict(x_test_scaled)\n",
    "\n",
    "# Calculate in-sample R2 and MSE\n",
    "in_sample_r2 = r2_score(y_train, y_train_pred)\n",
    "in_sample_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate out-of-sample R2 and MSE\n",
    "out_of_sample_r2 = r2_score(y_test, y_test_pred)\n",
    "out_of_sample_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"In-sample R2 (LASSO with scaled features):\", in_sample_r2)\n",
    "print(\"In-sample MSE (LASSO with scaled features):\", in_sample_mse)\n",
    "print(\"Out-of-sample R2 (LASSO with scaled features):\", out_of_sample_r2)\n",
    "print(\"Out-of-sample MSE (LASSO with scaled features):\", out_of_sample_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7052da",
   "metadata": {},
   "source": [
    "### High Dimension (up to degree 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c708e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R2 (LASSO with scaled features and interactions): 0.41047160894642964\n",
      "In-sample MSE (LASSO with scaled features and interactions): 1.531163904825487\n",
      "Out-of-sample R2 (LASSO with scaled features and interactions): 0.3789368062989372\n",
      "Out-of-sample MSE (LASSO with scaled features and interactions): 1.5967806928482196\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create polynomial features including all possible interactions up to degree 3\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "x_train_poly = poly_features.fit_transform(x_train)\n",
    "x_test_poly = poly_features.transform(x_test)\n",
    "\n",
    "# Create a StandardScaler object and fit it to the training data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_poly)\n",
    "x_test_scaled = scaler.transform(x_test_poly)\n",
    "\n",
    "# Create a LASSO Regression model\n",
    "model = Lasso(alpha=0.1)  # Adjust the alpha value for regularization strength\n",
    "\n",
    "# Fit the model using the scaled training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable for the training and testing data\n",
    "y_train_pred = model.predict(x_train_scaled)\n",
    "y_test_pred = model.predict(x_test_scaled)\n",
    "\n",
    "# Calculate in-sample R2 and MSE\n",
    "in_sample_r2 = r2_score(y_train, y_train_pred)\n",
    "in_sample_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate out-of-sample R2 and MSE\n",
    "out_of_sample_r2 = r2_score(y_test, y_test_pred)\n",
    "out_of_sample_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"In-sample R2 (LASSO with scaled features and interactions):\", in_sample_r2)\n",
    "print(\"In-sample MSE (LASSO with scaled features and interactions):\", in_sample_mse)\n",
    "print(\"Out-of-sample R2 (LASSO with scaled features and interactions):\", out_of_sample_r2)\n",
    "print(\"Out-of-sample MSE (LASSO with scaled features and interactions):\", out_of_sample_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ecb84",
   "metadata": {},
   "source": [
    "## LASSO (Cross-Validated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1acb4",
   "metadata": {},
   "source": [
    "### Moderate Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02eebfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R2 (LASSO with alpha tuning): 0.4044155048568142\n",
      "In-sample MSE (LASSO with alpha tuning): 1.5468932371640252\n",
      "Out-of-sample R2 (LASSO with alpha tuning): 0.36492012163751597\n",
      "Out-of-sample MSE (LASSO with alpha tuning): 1.6328182034785343\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LASSO Regression model with cross-validated alpha tuning\n",
    "model = LassoCV(cv=5)  # Adjust cv parameter for the number of cross-validation folds\n",
    "\n",
    "# Fit the model using the training data\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict the target variable for the training and testing data\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "# Calculate in-sample R2 and MSE\n",
    "in_sample_r2 = r2_score(y_train, y_train_pred)\n",
    "in_sample_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate out-of-sample R2 and MSE\n",
    "out_of_sample_r2 = r2_score(y_test, y_test_pred)\n",
    "out_of_sample_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"In-sample R2 (LASSO with alpha tuning):\", in_sample_r2)\n",
    "print(\"In-sample MSE (LASSO with alpha tuning):\", in_sample_mse)\n",
    "print(\"Out-of-sample R2 (LASSO with alpha tuning):\", out_of_sample_r2)\n",
    "print(\"Out-of-sample MSE (LASSO with alpha tuning):\", out_of_sample_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47246cd7",
   "metadata": {},
   "source": [
    "### High Dimension (up to degree 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4cf4fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R2 (LASSO with scaled features and interactions): 0.4874002455878892\n",
      "In-sample MSE (LASSO with scaled features and interactions): 1.3313595299041534\n",
      "Out-of-sample R2 (LASSO with scaled features and interactions): 0.3740535237823608\n",
      "Out-of-sample MSE (LASSO with scaled features and interactions): 1.6093358262376662\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create polynomial features including all possible interactions up to degree 3\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "x_train_poly = poly_features.fit_transform(x_train)\n",
    "x_test_poly = poly_features.transform(x_test)\n",
    "\n",
    "# Create a StandardScaler object and fit it to the training data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_poly)\n",
    "x_test_scaled = scaler.transform(x_test_poly)\n",
    "\n",
    "# Create a LASSO Regression model with cross-validated alpha tuning\n",
    "model = LassoCV(cv=5, max_iter=10000)  # Adjust cv parameter for the number of cross-validation folds\n",
    "\n",
    "# Fit the model using the scaled training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict the target variable for the training and testing data\n",
    "y_train_pred = model.predict(x_train_scaled)\n",
    "y_test_pred = model.predict(x_test_scaled)\n",
    "\n",
    "# Calculate in-sample R2 and MSE\n",
    "in_sample_r2 = r2_score(y_train, y_train_pred)\n",
    "in_sample_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate out-of-sample R2 and MSE\n",
    "out_of_sample_r2 = r2_score(y_test, y_test_pred)\n",
    "out_of_sample_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"In-sample R2 (LASSO with scaled features and interactions):\", in_sample_r2)\n",
    "print(\"In-sample MSE (LASSO with scaled features and interactions):\", in_sample_mse)\n",
    "print(\"Out-of-sample R2 (LASSO with scaled features and interactions):\", out_of_sample_r2)\n",
    "print(\"Out-of-sample MSE (LASSO with scaled features and interactions):\", out_of_sample_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92d662",
   "metadata": {},
   "source": [
    "## Post-LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc176f83",
   "metadata": {},
   "source": [
    "### Moderate Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cee9c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R2 (Post-LASSO): 0.40479298489512383\n",
      "In-sample MSE (Post-LASSO): 1.5459128199046985\n",
      "Out-of-sample R2 (Post-LASSO): 0.3620820157699305\n",
      "Out-of-sample MSE (Post-LASSO): 1.6401150980612154\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a StandardScaler object and fit it to the training data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Create a LASSO Regression model with cross-validated alpha tuning\n",
    "model = LassoCV(cv=5)  # Adjust cv parameter for the number of cross-validation folds\n",
    "\n",
    "# Fit the model using the scaled training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Perform post-LASSO inference\n",
    "X_train_scaled_with_intercept = sm.add_constant(x_train_scaled)\n",
    "model_sm = sm.OLS(y_train, X_train_scaled_with_intercept)\n",
    "results = model_sm.fit_regularized(alpha=model.alpha_, L1_wt=1)\n",
    "\n",
    "# Predict the target variable for the training and testing data\n",
    "X_test_scaled_with_intercept = sm.add_constant(x_test_scaled)\n",
    "y_train_pred = results.predict(X_train_scaled_with_intercept)\n",
    "y_test_pred = results.predict(X_test_scaled_with_intercept)\n",
    "\n",
    "# Calculate in-sample R2 and MSE\n",
    "in_sample_r2 = r2_score(y_train, y_train_pred)\n",
    "in_sample_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate out-of-sample R2 and MSE\n",
    "out_of_sample_r2 = r2_score(y_test, y_test_pred)\n",
    "out_of_sample_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"In-sample R2 (Post-LASSO):\", in_sample_r2)\n",
    "print(\"In-sample MSE (Post-LASSO):\", in_sample_mse)\n",
    "print(\"Out-of-sample R2 (Post-LASSO):\", out_of_sample_r2)\n",
    "print(\"Out-of-sample MSE (Post-LASSO):\", out_of_sample_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e884328a",
   "metadata": {},
   "source": [
    "### High Dimension (up to degree 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbc138a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample R2 (Post-LASSO with interactions): 0.48148956297653944\n",
      "In-sample MSE (Post-LASSO with interactions): 1.3467111635230267\n",
      "Out-of-sample R2 (Post-LASSO with interactions): 0.3608213065766117\n",
      "Out-of-sample MSE (Post-LASSO with interactions): 1.6433564366554272\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create polynomial features including all possible interactions up to degree 3\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "x_train_poly = poly_features.fit_transform(x_train)\n",
    "x_test_poly = poly_features.transform(x_test)\n",
    "\n",
    "# Create a StandardScaler object and fit it to the training data with polynomial features\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_poly)\n",
    "x_test_scaled = scaler.transform(x_test_poly)\n",
    "\n",
    "# Create a LASSO Regression model with cross-validated alpha tuning\n",
    "model = LassoCV(cv=5, max_iter=10000)  # Adjust cv parameter for the number of cross-validation folds\n",
    "\n",
    "# Fit the model using the scaled training data\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Perform post-LASSO inference\n",
    "X_train_scaled_with_intercept = sm.add_constant(x_train_scaled)\n",
    "model_sm = sm.OLS(y_train, X_train_scaled_with_intercept)\n",
    "results = model_sm.fit_regularized(alpha=model.alpha_, L1_wt=1)\n",
    "\n",
    "# Predict the target variable for the training and testing data\n",
    "X_test_scaled_with_intercept = sm.add_constant(x_test_scaled)\n",
    "y_train_pred = results.predict(X_train_scaled_with_intercept)\n",
    "y_test_pred = results.predict(X_test_scaled_with_intercept)\n",
    "\n",
    "# Calculate in-sample R2 and MSE\n",
    "in_sample_r2 = r2_score(y_train, y_train_pred)\n",
    "in_sample_mse = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate out-of-sample R2 and MSE\n",
    "out_of_sample_r2 = r2_score(y_test, y_test_pred)\n",
    "out_of_sample_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"In-sample R2 (Post-LASSO with interactions):\", in_sample_r2)\n",
    "print(\"In-sample MSE (Post-LASSO with interactions):\", in_sample_mse)\n",
    "print(\"Out-of-sample R2 (Post-LASSO with interactions):\", out_of_sample_r2)\n",
    "print(\"Out-of-sample MSE (Post-LASSO with interactions):\", out_of_sample_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e6017",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad90896",
   "metadata": {},
   "source": [
    "#### Best Performing Model is Cross-Validated LASSO with High Dimension up to degree 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35f78f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LassoCV(cv=5, max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LassoCV</label><div class=\"sk-toggleable__content\"><pre>LassoCV(cv=5, max_iter=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LassoCV(cv=5, max_iter=10000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create polynomial features including all possible interactions up to degree 3\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "x_train_poly = poly_features.fit_transform(x_train)\n",
    "x_test_poly = poly_features.transform(x_test)\n",
    "\n",
    "# Create a StandardScaler object and fit it to the training data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_poly)\n",
    "x_test_scaled = scaler.transform(x_test_poly)\n",
    "\n",
    "# Create a LASSO Regression model with cross-validated alpha tuning\n",
    "model = LassoCV(cv=5, max_iter=10000)  # Adjust cv parameter for the number of cross-validation folds\n",
    "\n",
    "# Fit the model using the scaled training data\n",
    "model.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f260261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filter the rows with missing birthorder values\n",
    "missing_rows = df[df['brthord'].isnull()]\n",
    "\n",
    "# Extract the features for the missing rows\n",
    "x_missing = missing_rows[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "\n",
    "# Create polynomial features with interactions up to degree 3\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "x_missing_interactions = poly_features.fit_transform(x_missing)\n",
    "\n",
    "# Create a new DataFrame with the original and interaction features\n",
    "missing_rows_interactions_df = pd.DataFrame(x_missing_interactions,\n",
    "                                            columns=poly_features.get_feature_names_out(x_missing.columns))\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "x_missing_scaled = scaler.fit_transform(missing_rows_interactions_df)\n",
    "\n",
    "# Predict the birthorder for the missing rows using the trained LASSO model\n",
    "birthorder_pred = model.predict(x_missing_scaled)\n",
    "\n",
    "# Fill the missing values with the predicted birthorder values\n",
    "missing_rows['brthord'] = birthorder_pred\n",
    "\n",
    "missing_rows['brthord'] = round(missing_rows['brthord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac84bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index of both DataFrames to match on \"id\" and \"year\" columns\n",
    "df = df.set_index([\"id\", \"year\"])\n",
    "missing_rows = missing_rows.set_index([\"id\", \"year\"])\n",
    "\n",
    "# Replace the missing values in df with values from missing_rows\n",
    "df[\"brthord\"] = df[\"brthord\"].combine_first(missing_rows[\"brthord\"])\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "207f635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('filled_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e46df",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f42986c",
   "metadata": {},
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a9dfec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features:  164\n",
      "Number of features LASSO used (non-zero coefficient):  10\n",
      "Number of features LASSO did not use (zero coefficient):  154\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Seperate independent and dependent variables\n",
    "x_brthord = new_df[[\"sibs\", \"age\", \"married\", \"female\", \"IQ\", \"black\", \"south\", \"urban\"]]\n",
    "y_brthord = new_df[['brthord']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_brthord, y_brthord, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create polynomial features including all possible interactions up to degree 3\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "x_train_poly = poly_features.fit_transform(x_train)\n",
    "x_test_poly = poly_features.transform(x_test)\n",
    "\n",
    "# Create a LASSO model\n",
    "model = LassoCV(cv=5, max_iter=100000)\n",
    "\n",
    "# Fit the model using the training data with polynomial features\n",
    "model.fit(x_train_poly, y_train)\n",
    "\n",
    "# Retrieve the coefficients\n",
    "coef = model.coef_\n",
    "\n",
    "# Count number of coefficients that are exactly zero\n",
    "n_zero_coef = np.sum(coef == 0)\n",
    "total_coef = len(coef)\n",
    "\n",
    "print(\"Total number of features: \", total_coef)\n",
    "print(\"Number of features LASSO used (non-zero coefficient): \", total_coef - n_zero_coef)\n",
    "print(\"Number of features LASSO did not use (zero coefficient): \", n_zero_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2661bd",
   "metadata": {},
   "source": [
    "### Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c629a68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durbin-Watson statistic: 2.0312742054642707\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Predict the outcomes for the training data\n",
    "y_train_pred = model.predict(x_train_poly)\n",
    "\n",
    "# Compute the residuals\n",
    "residuals = y_train.values.flatten() - y_train_pred\n",
    "\n",
    "# Perform the Durbin-Watson test\n",
    "dw_result = durbin_watson(residuals)\n",
    "\n",
    "print('Durbin-Watson statistic:', dw_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aec5ce",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc82b32",
   "metadata": {},
   "source": [
    "### Sibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae8ccecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Change in Sibs    Change in Prediction\n",
      "0        -0.200000  [-0.06736983671553709]\n",
      "1        -0.191837  [-0.13198988417737878]\n",
      "2        -0.183673  [-0.19386014238552507]\n",
      "3        -0.175510  [-0.25298061133997596]\n",
      "4        -0.167347  [-0.30935129104073167]\n",
      "5        -0.159184  [-0.36297218148779153]\n",
      "6        -0.151020  [-0.41384328268115644]\n",
      "7        -0.142857   [-0.4619645946208255]\n",
      "8        -0.134694   [-0.5073361173067994]\n",
      "9        -0.126531   [-0.5499578507390781]\n",
      "10       -0.118367   [-0.5898297949176614]\n",
      "11       -0.110204   [-0.6269519498425491]\n",
      "12       -0.102041   [-0.6613243155137414]\n",
      "13       -0.093878   [-0.6929468919312383]\n",
      "14       -0.085714     [-0.72181967909504]\n",
      "15       -0.077551    [-0.747942677005146]\n",
      "16       -0.069388    [-0.771315885661557]\n",
      "17       -0.061224   [-0.7919393050642722]\n",
      "18       -0.053061   [-0.8098129352132923]\n",
      "19       -0.044898    [-0.824936776108617]\n",
      "20       -0.036735   [-0.8373108277502463]\n",
      "21       -0.028571   [-0.8469350901381802]\n",
      "22       -0.020408   [-0.8538095632724185]\n",
      "23       -0.012245   [-0.8579342471529616]\n",
      "24       -0.004082   [-0.8593091417798093]\n",
      "25        0.004082   [-0.8579342471529616]\n",
      "26        0.012245   [-0.8538095632724185]\n",
      "27        0.020408   [-0.8469350901381802]\n",
      "28        0.028571   [-0.8373108277502463]\n",
      "29        0.036735    [-0.824936776108617]\n",
      "30        0.044898   [-0.8098129352132923]\n",
      "31        0.053061   [-0.7919393050642722]\n",
      "32        0.061224    [-0.771315885661557]\n",
      "33        0.069388    [-0.747942677005146]\n",
      "34        0.077551     [-0.72181967909504]\n",
      "35        0.085714   [-0.6929468919312383]\n",
      "36        0.093878   [-0.6613243155137414]\n",
      "37        0.102041   [-0.6269519498425491]\n",
      "38        0.110204   [-0.5898297949176612]\n",
      "39        0.118367   [-0.5499578507390781]\n",
      "40        0.126531   [-0.5073361173067994]\n",
      "41        0.134694   [-0.4619645946208255]\n",
      "42        0.142857  [-0.41384328268115644]\n",
      "43        0.151020  [-0.36297218148779153]\n",
      "44        0.159184  [-0.30935129104073145]\n",
      "45        0.167347  [-0.25298061133997596]\n",
      "46        0.175510  [-0.19386014238552507]\n",
      "47        0.183673  [-0.13198988417737878]\n",
      "48        0.191837  [-0.06736983671553709]\n",
      "49        0.200000                   [0.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Choose a random data point from the test set\n",
    "data_point = x_test.iloc[1, :]\n",
    "\n",
    "# Save the original prediction\n",
    "original_prediction = model.predict(poly_features.transform(data_point.values.reshape(1, -1)))\n",
    "\n",
    "# Save the original value of \"sibs\"\n",
    "original_sibs = data_point[\"sibs\"]\n",
    "\n",
    "# Create an array to hold the changes in \"sibs\" and the corresponding changes in the prediction\n",
    "changes = []\n",
    "\n",
    "# Change \"sibs\" by -10% to +10%\n",
    "for change in np.linspace(-0.10 * original_sibs, 0.10 * original_sibs, num=50):\n",
    "    data_point[\"sibs\"] += change\n",
    "    new_prediction = model.predict(poly_features.transform(data_point.values.reshape(1, -1)))\n",
    "    prediction_change = new_prediction - original_prediction\n",
    "    changes.append((change, prediction_change))\n",
    "\n",
    "# Convert the results to a DataFrame for easier viewing\n",
    "sensitivity_df = pd.DataFrame(changes, columns=[\"Change in Sibs\", \"Change in Prediction\"])\n",
    "\n",
    "print(sensitivity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a993ea6",
   "metadata": {},
   "source": [
    "### IQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "718b82d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Change in IQ     Change in Prediction\n",
      "0     -10.800000    [0.24009092022282996]\n",
      "1     -10.359184    [0.45404556817818476]\n",
      "2      -9.918367     [0.6423748976079933]\n",
      "3      -9.477551     [0.8058938911623783]\n",
      "4      -9.036735     [0.9456790684491414]\n",
      "5      -8.595918      [1.063027882614383]\n",
      "6      -8.155102     [1.1594200054542543]\n",
      "7      -7.714286     [1.2364805010578428]\n",
      "8      -7.273469     [1.2959448879811954]\n",
      "9      -6.832653     [1.3396260899524723]\n",
      "10     -6.391837     [1.3693832751082347]\n",
      "11     -5.951020     [1.3870925837608687]\n",
      "12     -5.510204      [1.394619744697143]\n",
      "13     -5.069388     [1.3937945800078977]\n",
      "14     -4.628571     [1.3863873984488702]\n",
      "15     -4.187755     [1.3740872773326551]\n",
      "16     -3.746939     [1.3584822329517945]\n",
      "17     -3.306122     [1.3410412795330078]\n",
      "18     -2.865306     [1.3230983767225506]\n",
      "19     -2.424490     [1.3058382656027099]\n",
      "20     -1.983673     [1.2902841932394342]\n",
      "21     -1.542857     [1.2772875257610954]\n",
      "22     -1.102041     [1.2675192499683854]\n",
      "23     -0.661224     [1.2614633634753478]\n",
      "24     -0.220408     [1.2594121533815437]\n",
      "25      0.220408     [1.2614633634753478]\n",
      "26      0.661224     [1.2675192499683854]\n",
      "27      1.102041     [1.2772875257610954]\n",
      "28      1.542857     [1.2902841932394342]\n",
      "29      1.983673     [1.3058382656027099]\n",
      "30      2.424490     [1.3230983767225506]\n",
      "31      2.865306     [1.3410412795330078]\n",
      "32      3.306122     [1.3584822329517945]\n",
      "33      3.746939     [1.3740872773326551]\n",
      "34      4.187755     [1.3863873984488702]\n",
      "35      4.628571     [1.3937945800078977]\n",
      "36      5.069388      [1.394619744697143]\n",
      "37      5.510204     [1.3870925837608687]\n",
      "38      5.951020     [1.3693832751082347]\n",
      "39      6.391837     [1.3396260899524723]\n",
      "40      6.832653     [1.2959448879811954]\n",
      "41      7.273469     [1.2364805010578428]\n",
      "42      7.714286     [1.1594200054542543]\n",
      "43      8.155102     [1.0630278826143833]\n",
      "44      8.595918     [0.9456790684491416]\n",
      "45      9.036735     [0.8058938911623785]\n",
      "46      9.477551     [0.6423748976079935]\n",
      "47      9.918367      [0.454045568178185]\n",
      "48     10.359184    [0.24009092022282985]\n",
      "49     10.800000  [8.881784197001252e-16]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Choose a random data point from the test set\n",
    "data_point = x_test.iloc[0, :]\n",
    "\n",
    "# Save the original prediction\n",
    "original_prediction = model.predict(poly_features.transform(data_point.values.reshape(1, -1)))\n",
    "\n",
    "# Save the original value of \"exper\"\n",
    "original_IQ = data_point[\"IQ\"]\n",
    "\n",
    "# Create an array to hold the changes in \"IQ\" and the corresponding changes in the prediction\n",
    "changes = []\n",
    "\n",
    "# Change \"IQ\" by -10% to +10%\n",
    "for change in np.linspace(-0.10 * original_IQ, 0.10 * original_IQ, num=50):\n",
    "    data_point[\"IQ\"] += change\n",
    "    new_prediction = model.predict(poly_features.transform(data_point.values.reshape(1, -1)))\n",
    "    prediction_change = new_prediction - original_prediction\n",
    "    changes.append((change, prediction_change))\n",
    "\n",
    "# Convert the results to a DataFrame for easier viewing\n",
    "sensitivity_df = pd.DataFrame(changes, columns=[\"Change in IQ\", \"Change in Prediction\"])\n",
    "\n",
    "print(sensitivity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48221b8",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41e342cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Change in Age   Change in Prediction\n",
      "0       -4.300000  [0.12163875125805013]\n",
      "1       -4.124490   [0.2463641083596717]\n",
      "2       -3.948980   [0.3723834836928348]\n",
      "3       -3.773469   [0.4981478372051309]\n",
      "4       -3.597959   [0.6223298589382475]\n",
      "5       -3.422449   [0.7438031212275742]\n",
      "6       -3.246939   [0.8616222005669465]\n",
      "7       -3.071429   [0.9750037691385218]\n",
      "8       -2.895918   [1.0833086560077902]\n",
      "9       -2.720408   [1.1860248779837215]\n",
      "10      -2.544898   [1.2827516401440437]\n",
      "11      -2.369388   [1.3731843060256594]\n",
      "12      -2.193878   [1.4571003374801947]\n",
      "13      -2.018367   [1.5343462041946825]\n",
      "14      -1.842857   [1.6048252628773814]\n",
      "15      -1.667347    [1.668486606108729]\n",
      "16      -1.491837   [1.7253148808574281]\n",
      "17      -1.316327    [1.775321076661671]\n",
      "18      -1.140816   [1.8185342834754923]\n",
      "19      -0.965306    [1.854994419180264]\n",
      "20      -0.789796   [1.8847459267613191]\n",
      "21      -0.614286   [1.9078324411497103]\n",
      "22      -0.438776    [1.924292425729108]\n",
      "23      -0.263265    [1.934155778507827]\n",
      "24      -0.087755   [1.9374414079559894]\n",
      "25       0.087755    [1.934155778507827]\n",
      "26       0.263265    [1.924292425729108]\n",
      "27       0.438776   [1.9078324411497103]\n",
      "28       0.614286   [1.8847459267613191]\n",
      "29       0.789796    [1.854994419180264]\n",
      "30       0.965306   [1.8185342834754923]\n",
      "31       1.140816    [1.775321076661671]\n",
      "32       1.316327   [1.7253148808574281]\n",
      "33       1.491837    [1.668486606108729]\n",
      "34       1.667347   [1.6048252628773814]\n",
      "35       1.842857   [1.5343462041946825]\n",
      "36       2.018367   [1.4571003374801947]\n",
      "37       2.193878   [1.3731843060256594]\n",
      "38       2.369388   [1.2827516401440435]\n",
      "39       2.544898   [1.1860248779837215]\n",
      "40       2.720408   [1.0833086560077902]\n",
      "41       2.895918   [0.9750037691385218]\n",
      "42       3.071429   [0.8616222005669465]\n",
      "43       3.246939   [0.7438031212275742]\n",
      "44       3.422449   [0.6223298589382475]\n",
      "45       3.597959   [0.4981478372051309]\n",
      "46       3.773469   [0.3723834836928348]\n",
      "47       3.948980   [0.2463641083596717]\n",
      "48       4.124490  [0.12163875125805013]\n",
      "49       4.300000                  [0.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Choose a random data point from the test set\n",
    "data_point = x_test.iloc[0, :]\n",
    "\n",
    "# Save the original prediction\n",
    "original_prediction = model.predict(poly_features.transform(data_point.values.reshape(1, -1)))\n",
    "\n",
    "# Save the original value of \"age\"\n",
    "original_age = data_point[\"age\"]\n",
    "\n",
    "# Create an array to hold the changes in \"age\" and the corresponding changes in the prediction\n",
    "changes = []\n",
    "\n",
    "# Change \"age\" by -10% to +10%\n",
    "for change in np.linspace(-0.10 * original_age, 0.10 * original_age, num=50):\n",
    "    data_point[\"age\"] += change\n",
    "    new_prediction = model.predict(poly_features.transform(data_point.values.reshape(1, -1)))\n",
    "    prediction_change = new_prediction - original_prediction\n",
    "    changes.append((change, prediction_change))\n",
    "\n",
    "# Convert the results to a DataFrame for easier viewing\n",
    "sensitivity_df = pd.DataFrame(changes, columns=[\"Change in Age\", \"Change in Prediction\"])\n",
    "\n",
    "print(sensitivity_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
